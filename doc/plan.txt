1   extract ocr stuff out into its own libraries.

2   add module for other abilities such as reading large amounts of text and matching an answer


YOUR FREE API KEY: 9642dd40-1c73-11e6-a057-97f4c970893c
EXAMPLE : curl -X GET -H "api-key: 9642dd40-1c73-11e6-a057-97f4c970893c" "http://api.cortical.io/rest/retinas"


3   I still have to do the modified verison and test it. this is where it tests
    the answers with added text to match the longest one.

4    I have to make it faster - the main problem is finding the term that most
    matches the question. so I need to speed that up. quite a bit. and give some
    small indicator that its still thinking about it. a mouse move is the only
    real option. perhaps order the terms alphanumerically, then if the one we're
    about to check is the same as the one we just did then skip it. We can also
    say if its longer than our closest option like if our score is 22 and its
    length makes it have 22+ insertions then skip it, this could also be if its
    too short. that should help a bit.

5   try it with db2. Also make a DB from quizlet https://quizlet.com/24289043/comptia-network-acronyms-and-vocabulary-flash-cards/



I'm unfamiliar with string similarity algorithms except for Levenshtein Distance because that's what I'm using and it has turned out to be less than ideal.

So I've kind of got an idea of a recursive algorithm I'd like to implement but I want to know if it exists already so I can leverage other's expertise.

Here's the algorithm by example:

> string 1: "Paul Johnson"
>
> string 2: "John Paulson"

step 1: find all longest matches

> Match 1: "Paul"
>
> Match 2: "John"
>
> Match 3: "son"
>
> Match 4: " "

step 2: Calculate scores for each match with this formula: ((match.len/string.len)*match.len) This allows longer strings to be weighted more

> Match 1: (4/12)*4 = 1.333...
>
> Match 2: 1.333...
>
> Match 3: .75
>
> Match 4: .083





New Algorithm:

Find ALL Longest Common Substrings

up vote
0
down vote
favorite
This program returns the score of how similar one string is to another according to the following rules:

isolate only the longest (non-overlaping) matching substrings.
score every longest substring found with this formula:
(substring.length / ((string1.length + string2.length) / 2)) * substring.length

return score.
STEP 1 Example:

string1 = ABCD

string2 = ZBCA

Deconstructing string1: (list of substring in order)

ABCD

ABC

AB

A (also found in string2)

BCD

BC (also found in string2)

B (also found in string2 but ignored - part of a longer substring)

CD

C (also found in string2 but ignored - part of a longer substring)

D

Matches: A, BC

STEP 2 Example: (substring.length / ((string1.length + string2.length) / 2)) * substring.length

A: (1/((4+4)/2)) * 1 = .25

BC: (2/((4+4)/2)) * 2 = 1

Step 3 Example: .25 + 1 = 1.25, Return 1.25
Here's an example of longer strings of variable length:

string1 = Approximate This

string2 = Appropriate That Thing

Approximate This

Approximate Thi

...

A

pproximate This

... ...

s

Matches: Appro, ate, i, th, i (2 single i's in both, not two th's in both)

Appro: (5/((16+22)/2)) * 5 = 1.3157894736842105263157894736842

ate: (3/((16+22)/2)) * 3 = 0.47368421052631578947368421052632

i: (1/((16+22)/2)) * 1 = 0.05263157894736842105263157894737

th: (2/((16+22)/2)) * 2 = 0.21052631578947368421052631578947

i: (1/((16+22)/2)) * 1 = 0.05263157894736842105263157894737

Return: 2.1052631578947368421052631578947





6   Things to do now:
      1. Modified is no longer important now that we're not using levenshtein
          Distance. So re-factor everything to either take it out or something.
      2. Backwards search needs a second best option.
      3. would be cool to adjust the amount of score's we let through. right now
          we're simply taking the best score and any tie breakers. but it would
          be cool to say take the top 3% of scores, or even better, the scores
          at the top of the S curve.
      4. make a module that gets a specific area on the screen and you press to
          refresh. that way we can use this for gathering vocab from cards or
          something.
      5. if you feel radical go all the way and implement iris to make a real
          semantic network to use. of course do that in tcl, not autoit.
          a. If you did make a semantic network you might could use that to read
              the text and automate the process of develop this database.
              i. If you did that you could package this up and sell it for $250
                  a pop, with free updates.
              ii. I'd rather make a monthly fee though, so don't do this except
                  on a little side business.
      6. change tooltip to move mouse.














Retina Notes
      # the idea behind retina is to create a semantic map of the entire text by feeding in the entire text to this module
      # that way I can later reference a particular word, a particular phrase or sentance
      # that should return the following:
        # 1. a similarity score.
        # 2. key words extraction.
        # 3. a possible proximity score if semantic similarity isn't good enough.

      # conceptual steps:
        # 1. chop up the text starting at 2 word pairs and going up to paragraphs or chapers.
        # 2. Reorganize each and every word-size-level according to levenshtein distance or  alllcs.
          # a. do this by finding the most common substring or the most common longest substring. or rather...
          # a. do this by finding the context which has the highest score with the most other contexts and place that one in the middle of the grid. or...
          # a. do this by taking the first context in the list and placing it at the beginning... OR RATHER....
          # a. Don't do this at all. Since its comming from a text book we can assume its already organized in a semantic logical way.
        # 3. build semantic maps of each word and each collection of words at every level of contextual representation.

      #  Official Retina: http://www.cortical.io/static/downloads/semantic-folding-theory-white-paper.pdf
      #  • Definition	of	a	reference	text	corpus	of	documents	that	represents	the	Semantic
      #  Universe the	system	is	supposed	to	work	in.	The	system	will	know	all	vocabulary
      #  and	its	practical	use	as	it	occurs	in	this	Language	Definition	Corpus	(LDC).
      #  • Every	document	from	the	LDC	is	cut	into	text	snippets	with	each	snippet
      #  representing	a	single	context.
      #  • The	reference	collection	snippets	are	distributed	over	a	2D	matrix	(e.g.	128x128
      #  bits)	in	a	way	that	snippets	with	similar	topics	(that	share	many	common	words)
      #  are	placed	closer	to	each	other	on	the	map,	and	snippets	with	different	topics
      #  (few	common	words)	are	placed	more	distant	to	each	other	on	the	map.	That
      #  produces	a	2D	semantic	map.
      #  • In	the	next	step, a	list	of	every	word contained	in	the	reference	corpus is	created.
      #  • By	going	down	this	list	word	by	word,	all	the	contexts	a	word	occurs	in	are	set	to
      #  1	in	the	corresponding	bit-position	of	a	2D	mapped	vector.	This	produces	a	large,
      #  binary,	very	sparsely	filled	vector	for	each	word.	This	vector	is	called	the
      #  Semantic	Fingerprint of	the	word.	The	structure	of	the	2D	map	(the	Semantic
      #  Universe) is	therefore	“folded	into”	each	representation	of	a	word	(Semantic
      #  Fingerprint).	The	list	of	words	with	their	fingerprints	is	stored	in	a	database that
      #  is	indexed	to	allow	for	fast	matching.	The	system	that	converts	a	given	word	into
      #  a	fingerprint	is	called	Retina, as	it	acts	as	a	“sensorial	organ	for	text”.	The
      #  fingerprint	database	is	called	Retina Database (Retina	DB).



      # Reorganize brainstorm: contexts:
      # lets say having the same letters in the wrong order is +1,
      # in the same order is +1
      #---------------------
      #     ab  ac  yz  zy  cy  cz  xx  xy  zx
      # ab  4
      # ac  2   4
      # yz  0   0   4
      # zy  0   0   2   4
      # cy  0   1   1   2   4
      # cz  0   1   2   1   2   4
      # xx  0   0   0   0   0   0   4
      # xy  0   0   1   2   2   0   2   4
      # zx  0   0   1   2   0   1   2   1   4
      #---------------------
      # might produce something like this:
      # xx  xy  cy
      # yz  zy  cz
      # zx  ab  ac
      #---------------------
      # why try to stuff it into a 2d surface? no real need right? just keep it in the multi-dimentional grid like above.
      # anyway.

      # to query this database for question and answers you might have this:
      # 1. determine the approapriate size of the context level.
      # 2. get the top 4% contexts of matching by alllcs.
      # 3. take that 4% SDR and find the word in the database that matches that SDR best. (out of your options).

      # the goal with the aboves strategy is to create a robust way to filter noise.
      # noise as in "Of the following ... which ... ?" in other words the parts of the question.
      # its trying to match the definition to a set of contexts, not trying to understand the question.































7. Turn run into "manage program" then give the option of Run or Kill. Could do the same with a Manage input button then let them choose keyboard or mouse. that would free up some realestate on the behavior screen. Also Finish the OCR trigger and behavior. You made the library already - use it. This means making a module that knows how to use the tesseract_stdout.au3 thing to check a region of the screen. You could also make a variable management so they can extract some data away. To check for the OCR text we'll also want to match it not exactly but using some metric like allls or even just edit distance. this is doen

8. Finish the import plugin portion (including Encrypting and decrypting the plugin code and running it in a fashion that is secure, and creating the payment structure and making it impossible* to cheat: that is require a code they get upon buying it. save these codes in the online DB along with the email and name on the card that bought it. These plugins will require the use of sqlite so be sure to include a library for that and teach your program how to use it.)

9. list of things to finish in order:
DONE	a. make the import plugin code functional.
NO		1. plugin requirements: needs a Plugin_Main() function to start the plugin.
NO		2. possible other features such as a pause command or something.
???	b. make the plugins cryptographically secure as possible.
	c. make all the approapriate libraries for plugins to be able to work robustly. (including gui stuff)
???	d. make the pro version of the program secure so they need to put in a code.
???	e. make the website and webapp for security with login and stuff.
	f. make tutorials and promo vids. (10-30) put on youtube.
	g. make plugin for local cheating. (ai creates the semantic db)
	h. make plugin for remote cheating. (to our databases of answers)
	i. make plugin to combine the two? no they can just run two instances.

10. We've made the import plugin code functional, but haven't done much else. we need to tie in the website to it's cryptographically secure peice. Then finish the website so you can buy it. then make a plugin for cheating, then make the db for it then make a vids









11. make it windows 10 compatible:


Aug 3, 2016
2:41:03 PM: Jordan Miller: I wanted to ask you about how you got the ipdisplay to work with the zoomed in windows 10? I've been learning autoit but I don't know how to do that
2:42:14 PM: Cliff Frazier: Let me say first that my programming skills are pretty miniscule, so I'm sure there's a much better way to do it than the way I did it.
2:43:14 PM: Cliff Frazier: It seems to me the issue is all about DPI.  When you use the zoom settings you're really changing the display DPI, so I've built a way to detect the DPI setting and then adjust the various font sizes, windows sizes and text location settings
2:44:06 PM: Cliff Frazier: There is also a setting in AutoIt when you compile to allow for smoother fonts and such for highdpi displays
2:44:41 PM: Cliff Frazier: I can't remember what it is, but I had to research that.  The layout was fine, but the fonts were 'fuzzy' before I made that change
2:46:12 PM: Cliff Frazier: If you're interested in learning a language, I wouldn't take too much time with AutoIt, there are a lot more useful languages.  I learned it because it helped me quickly fix some shortcomings in ZENworks many many years ago.
2:47:03 PM: Jordan Miller: yeah I'm going to lean python too
2:47:24 PM: Cliff Frazier: If it had existed back then, I would have learned powershell
2:47:55 PM: Cliff Frazier: it would have been much more useful for the Windows stuff I was doing with ZCM
2:49:13 PM: Cliff Frazier: I can get more specific about the AutoIt stuff, but it's pretty clugey, it works, but it's not something I'm super proud to share.
2:54:15 PM: Jordan Miller: well I'm working on a project of my own - you know to learn it - and I thought it'd be cool to make it compatible with windows 10 like you did for ipdisplay. I think if I knew how to detect the dpi like you mentioned that'd be enough
2:54:20 PM: Jordan Miller: how did you do that part?
2:58:46 PM: Cliff Frazier: I find the logged in users SID using an autoit function called _Security_LookupAccountName()  I then plug that SID into a registry key that contains the users DPI settings -HKEY_USERS\[SID]\Control Panel\Desktop\WindowMetrics\AppliedDPI.  Then I have a simple if/then/else routine to apply different sizes and positions based on the returned DPI setting.
2:59:58 PM: Jordan Miller: ah interesting
3:01:15 PM: Cliff Frazier: It's far from elegant, but it seems to work.  It's a pain to update, if I had more time and better skills I'd update it.  But I just don't have time anymore.  There probably won't be too many updates to that program anymore
3:02:01 PM: Jordan Miller: thats understandable, thanks, I'll let you know if that's the way I end up doing things in my project
3:02:20 PM: Cliff Frazier: you're welcome


that means
	1. get the proper username

	2. get the SID:

		#include <Security.au3>

		Local $aArrayOfData = _Security__LookupAccountName(@UserName)

		; Print returned data if no error occured
		If IsArray($aArrayOfData) Then
    			ConsoleWrite("SID String = " & $aArrayOfData[0] & @CRLF)
    			ConsoleWrite("Domain name = " & $aArrayOfData[1] & @CRLF)
			ConsoleWrite("SID type = " & _Security__SidTypeStr($aArrayOfData[2]) & @CRLF)
		EndIf

	3. get the value of the applieddpi

		HKEY_USERS\[SID]\Control Panel\Desktop\WindowMetrics\AppliedDPI

		96 is normal (0x00000060)
		288 is 300% zoom in on surface book (0x00000120)
		192 is 200% (0x000000c0)

    this has all been done in temp/step2.au3
    See also capturescreenpart.au3 for how *2 captures the right screen area.
    that one is working correctly.

	4. adjust where the image is taken and how big it is according to that. (don't know how)




12. Semantic approach:

So I want to record my latest design thoughts on how to implement the semantic/
retina approach. We should read in the whole Text book which would probably be
around 20k words. we then chop up the text in the largest 3 sizes possible, for
instance:

20k/1024  = ~20 words per snippet.
20k/512   = ~40 words
20k/256   = ~80 words

if the book is 40k you'd use 2048, 1024, 512, for example. Anyway, the database
then holds a sparse distributed representation of each word and where it appears
in the text up to 5% sparsity. (you can just record indices if you want). If the
word appears more than the sparsity of 5% then you take the ones that have it
occurring more than once, and if you must, choose the others at random.

1024
Hello 01110001010000000000000010000001000000000000001000000000010000000000000...
World 00000110000001001000000000000000000000000010110101000000000000000000001...
512
Hello 00001000100000000101000000001000000000000000000000010001000000001000000...
World 00010101000000000000010100001111000000000000000000000000000000000000010...
256
Hello 00100100001010000000000000000000100000000000000000000000000000000000000...
World 00000000001011001001000101001000100000000000000000000000000000000000000...

Also make a table to record the relative rareness of each word.

Hello   9
World   62

Now you've done everything you need to do in the pre-processing portion. When a
question comes in you take every single word in the question and every word in
all available answers and find the highest scored answer matching the question.

Let me give you an example in order to explain the steps.

    What is water?

    A. Bottled
    B. solid
    C. H2O

If the textbook never mentions H2O then it will have no memory of H2O and wont
choose it because it's score is 0. Lets say the text book has mentioned "solid"
and "Bottled." We're going to check the words against each other and count the
number of overlapping bits. (Remember to remove all punctuation in textbook and
questions and answers). PS. you would do this for all sizes too (which I don't
show here); we do this so we can approximate taking higher level concepts into
account.

What  Bottled   0
is    Bottled   5
water Bottled   3

Score: 8

What  solid     1
is    solid     10
water solid     0

Score: 11

Then we have to take into account the relative scarcity of each word:

What      14
is        70
water     56
Bottled   8
solid     7

Now we have to modify the scores according to relative rarity. you can do this
by dividing the smallest number by each of the other numbers.

What      0.5     (7/14)
is        0.1     (7/70)
water     0.125   (7/56)
Bottled   0.875   (7/8)
solid     1       (7/7)

is    Bottled   5 * 0.1 * 0.875 =   0.4375
water Bottled   3 * 0.5 * 0.875 =   1.3125

Score: 1.75

What  solid     1  * 0.5 * 1 =      0.5
is    solid     10 * 0.1 * 1 =      1

Score: 1.5

Now we have the final scores of each answer. The rarity score is the process by
which we approximately determine what the key words are and weight them
accordingly. Now there are other approaches such as bag of words or even search
algorithms that might work very well in some cases where the exact definition is
given, but I'm hoping this approach will work more generally, so they it can be
somewhat helpful even on story questions or questions that require an extra step
of logic. we'll have to experiment to see if it can be helpful more generally.

Also some might suggest to take the relative rarity and turn it into a
logarithmic scale so that the rarest words are really important. But there are
two things to consider with that - the key words are not the rarest words, they
are somewhat rare, but they probably show up more than once. Secondly, I think
whatever logarithmic weights would show up in the overlap of the bits because
it becomes exponentially less likely to have more and more bits sharing; though
taking larger scales lessens the steepness of that curve.

At this point I think you should experiment and find out if this is a viable
strategy.




13. 12 was inadequate, or perhaps just incomplete. we may need to also index
every combination of multiple words too, but only caring about those that show
up next to each other in the text.

for example: taking the above sentence as the text we'd have each word indexed
with an sdr, but we'd also have:

we may
may need
need to
...
in the
the text
we may need
may need to
...
we may need to also index
...
ect.

we'd probably want to go up to like 10 words long or something. we'd also need
to weight the longer matches much higher than the smaller matches. But this is
essentially searching for the location of the longest common substring. So I was
thinking we could make a module called searchdistance that maybe leverages alllcs
or it just searches for the longest string, if its not found then it searches
for parts of the string, if those aren't found then it searches for permutations
of the string until it finds the index of the closest string. then it searches
around that string to find the closest thing to the answers. which ever answer
is the most accurate and closest to it wins. that would probably require regexp
usage. or lsearch with different permutations of words since we're getting rid
of punctuation and everything is lowercase. if there are 10 words in the question
we could do 1-10, then 2-10 then 1-9 then 2-9 then 3-10 then 1-8 etc. until you have just
1 and just 10. that could be a good approximation to the best match to the
question, except it would undoubtedly sometimes catch things you didn't want caught.
anyway. we'll have to think more about this. I've run out of time.
